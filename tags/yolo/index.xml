<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>YOLO on ML Journey</title><link>https://fabiogeraci.github.io/mljourney/tags/yolo/</link><description>Recent content in YOLO on ML Journey</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Fri, 16 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://fabiogeraci.github.io/mljourney/tags/yolo/index.xml" rel="self" type="application/rss+xml"/><item><title>YoloV5 Deployment Optimization with Neural Magic</title><link>https://fabiogeraci.github.io/mljourney/posts/second-blog/</link><pubDate>Fri, 16 Sep 2022 00:00:00 +0000</pubDate><guid>https://fabiogeraci.github.io/mljourney/posts/second-blog/</guid><description>Table Of Contents ğŸ”¥ Motivation: ğŸ”« What: ğŸš’ Why: â›³ How: ğŸ”© Setting Up Table Resume Prediction Speed Table Resume Metrics ğŸ™ Comments &amp;amp; Feedback Follow me ğŸ‘‡ ğŸ”¥ Motivation: Link to heading As architecture get more complex, for improved prediction results, the generated model are larger and larger, therefore the need to reduce their size and allow the model to run prediction on CPU. This enables developer to reduce infostructres costs as well as opening the door for edge deployment.</description></item></channel></rss>